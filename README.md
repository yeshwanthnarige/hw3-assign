# hw3-assign

Student Name : Yeshwanth Narige   
ID : 700764035

# Deepâ€‘Learning Examples

This repository contains four selfâ€‘contained TensorFlowÂ Keras projects demonstrating:

1. **Basic Autoencoder** for MNIST reconstruction  
2. **Denoising Autoencoder** for noise removal on MNIST  
3. **LSTMâ€‘based Text Generation** (Shakespeare)  
4. **Sentiment Classification** (IMDB reviews)  

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ ae_basic/               # Q1: Basic Autoencoder  
â”‚Â Â  â”œâ”€â”€ train.py            # definition & training script  
â”‚Â Â  â””â”€â”€ visualize.py        # original vs. reconstructed  
â”‚
â”œâ”€â”€ ae_denoise/             # Q2: Denoising Autoencoder  
â”‚Â Â  â”œâ”€â”€ train.py            # noisyâ†’clean training  
â”‚Â Â  â””â”€â”€ compare.py          # noisy vs. denoised vs. clean  
â”‚
â”œâ”€â”€ rnn_textgen/            # Q3: Text Generation with LSTM  
â”‚Â Â  â”œâ”€â”€ train.py            # data prep & model training  
â”‚Â Â  â””â”€â”€ generate.py         # sampling with temperature  
â”‚
â””â”€â”€ rnn_sentiment/          # Q4: Sentiment Classification  
    â”œâ”€â”€ train.py            # IMDB data prep & model  
    â””â”€â”€ evaluate.py         # confusion matrix & report  
```

---

## ğŸ”§ Requirements

- PythonÂ 3.8+  
- TensorFlowÂ 2.x  
- NumPy, Matplotlib  
- scikitâ€‘learn (for Q4)  

Install via:

```bash
pip install tensorflow numpy matplotlib scikit-learn
```

---

## 1. Basic Autoencoder (Q1)

**Objective:** Encode 28Ã—28 MNIST images to a 32â€‘dim latent vector and reconstruct them.

- **Architecture**  
  - Encoder: 784 â†’Â 32 (ReLU)  
  - Decoder: 32Â â†’Â 784 (Sigmoid)  
- **Loss:** Binary crossâ€‘entropy  
- **Usage:**  
  ```bash
  cd ae_basic
  python train.py      # trains model on MNIST
  python visualize.py  # plots original vs. reconstructed
  ```
- **Insights:**  
  - Smaller latent dims (16) blur details  
  - Larger dims (64) give sharper reconstructions but risk overfitting  

---

## 2. Denoising Autoencoder (Q2)

**Objective:** Train the same 784Â â€“Â 32Â â€“Â 784 autoencoder, but feed it **noisy** MNIST and teach it to recover the **clean** images.

- **Noise:** Gaussian, Î¼=0, Ïƒ=0.5  
- **Training:** Noisy inputs â†’ Clean targets  
- **Usage:**  
  ```bash
  cd ae_denoise
  python train.py      # trains denoising AE & basic AE
  python compare.py    # visualize noisy vs. denoised vs. basic AE
  ```
- **Comparison:**  
  - Basic AE reproduces noise if fed noisy input  
  - Denoising AE learns to filter out noise, producing cleaner digits  

---

## 3. LSTM Text Generation (Q3)

**Objective:** Train a characterâ€‘level LSTM on Shakespeareâ€™s sonnets to predict the next character, then sample new text.

- **Data:** `shakespeare.txt` (downloaded automatically)  
- **Model:**  
  1. Embedding â†’ 256-d  
  2. LSTM â†’ 1024 units  
  3. Dense â†’ vocab size  
- **Generation:**  
  - Seed with a prompt (e.g. â€œROMEO: â€œ)  
  - Temperature scaling to control creativity  
- **Usage:**  
  ```bash
  cd rnn_textgen
  python train.py       # trains the RNN (20 epochs)
  python generate.py    # sample new text with chosen temperature
  ```
- **Tip:**  
  - **T<1** â†’ more conservative, repetitive  
  - **T>1** â†’ more random, creative (but risk gibberish)  

---

## 4. Sentiment Classification (Q4)

**Objective:** Classify IMDB movie reviews as positive or negative using an LSTM.

- **Data:** `tensorflow.keras.datasets.imdb` (topÂ 10Â 000 words)  
- **Preprocessing:** pad/truncate each review to 200 tokens  
- **Model:**  
  1. Embedding â†’ 128â€‘d  
  2. LSTM â†’ 64 units + Dropout  
  3. Dense â†’ 1 (sigmoid)  
- **Evaluation:**  
  - Confusion matrix  
  - Precision, recall, F1â€‘score via `sklearn.metrics`  
- **Usage:**  
  ```bash
  cd rnn_sentiment
  python train.py      # trains & validates model
  python evaluate.py   # computes confusion matrix & report
  ```
- **Precisionâ€“Recall Tradeâ€‘off:**  
  - **Precision**: Of predicted positives, how many are true?  
  - **Recall**: Of true positives, how many did we catch?  
  - Adjusting the decision threshold lets you balance false positives vs. false negatives depending on application needs.

---

## ğŸ“– References

- [TensorFlow Keras Guide](https://www.tensorflow.org/guide/keras)  
- [Imdb Dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)  
 
